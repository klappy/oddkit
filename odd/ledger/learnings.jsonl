{"id":"learn-20260129-0001","timestamp":"2026-01-29T14:30:00Z","summary":"Freshness checks must compare against oddkit canon_target, not arbitrary versions, to avoid wasted updates","trigger":"policy","impact":"Derived prompts that update to intermediate versions create churn and confusion. Canon-target-first ensures updates only happen to the authoritative commit.","confidence":0.9,"sources":["klappy://canon/agents/odd-epistemic-guide","oddkit_policy_version"],"evidence":[{"type":"artifact","ref":"~/.cursor/agents/odd-epistemic-guide.md - Freshness Check section"}],"candidate_targets":["klappy://canon/agents/odd-epistemic-guide","klappy://canon/agents/odd-scribe"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260129-0002","timestamp":"2026-01-29T18:19:59Z","summary":"Comprehensive ODD agent documentation exists but was stale and lacked discoverability—users had no way to find subagents or setup instructions","trigger":"friction","impact":"Users cannot discover or configure ODD subagents (Epistemic Guide, Scribe) without explicit guidance, blocking adoption of core ODD capabilities","confidence":0.85,"sources":["docs/getting-started/agents.md","docs/getting-started/ledger.md","docs/getting-started/odd-agents-and-mcp.md"],"evidence":[{"type":"artifact","ref":"docs/getting-started/agents.md - setup instructions"},{"type":"artifact","ref":"docs/getting-started/odd-agents-and-mcp.md - system overview"}],"candidate_targets":["klappy://canon/odd/getting-started"],"proposed_escalation":"candidate-doc"}
{"id":"learn-20260130-0001","timestamp":"2026-01-30T01:30:00Z","summary":"The missing piece was agent role boundaries, not new principles—the map existed, we needed agents that knew how to use it without stepping on each other","trigger":"friction","impact":"Repeated mid-conversation steering was required because agents had overlapping or undefined responsibilities. Explicit role separation eliminates this drift.","confidence":0.95,"sources":["session-conversation-2026-01-30","klappy://canon/agents/odd-map-navigator","klappy://canon/agents/odd-mode-selector"],"evidence":[{"type":"artifact","ref":"klappy://canon/agents/odd-map-navigator"},{"type":"artifact","ref":"klappy://canon/agents/odd-mode-selector"},{"type":"artifact","ref":"klappy://canon/agents/odd-instruction-sync"},{"type":"artifact","ref":"klappy://canon/agents/odd-implementation-guide"}],"candidate_targets":["klappy://canon/agents"],"proposed_escalation":"none"}
{"id":"learn-20260130-0002","timestamp":"2026-01-30T01:32:00Z","summary":"Mode selector is MCP action routing with confidence, not a posture taxonomy—it picks the next action and asks minimum questions if unclear","trigger":"policy","impact":"Clarifies that mode selection reuses epistemic modes and MCP actions rather than inventing new concepts. Prevents future drift toward posture doctrine.","confidence":0.9,"sources":["klappy://canon/epistemic-modes","klappy://canon/agents/odd-mode-selector"],"evidence":[{"type":"artifact","ref":"klappy://canon/agents/odd-mode-selector - Purpose section"}],"candidate_targets":["klappy://canon/agents/odd-mode-selector"],"proposed_escalation":"none"}
{"id":"learn-20260130-0003","timestamp":"2026-01-30T01:34:00Z","summary":"Baseline cache auto-refreshes on any oddkit call via ensureBaselineRepo git fetch/pull—agents naturally get updates when oddkit is invoked","trigger":"policy","impact":"Understanding this flow is critical for the natural agent update path: push to klappy.dev → any oddkit call refreshes cache → sync-agents sees new files.","confidence":0.95,"sources":["oddkit/src/baseline/ensureBaselineRepo.js"],"evidence":[{"type":"artifact","ref":"oddkit/src/baseline/ensureBaselineRepo.js - lines 161-178"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260130-0004","timestamp":"2026-01-30T01:42:00Z","summary":"Cursor rules are the right place for universal safety constraints that apply when no agent is active, an agent goes off-script, or the interaction is quick and no one invoked a role","trigger":"policy","impact":"Agent role docs steer when an agent is selected; Cursor rules apply to every session. Keeping Compass in Cursor rules ensures the two failure modes—unjustified certainty and answering canon from memory—are covered even when role docs are missing or ignored.","confidence":0.9,"sources":["session-conversation-2026-01-30","oddkit/.cursor/rules/oddkit-compass.mdc"],"evidence":[{"type":"artifact","ref":"oddkit/.cursor/rules/oddkit-compass.mdc"}],"candidate_targets":["oddkit://docs/MCP.md"],"proposed_escalation":"none"}
{"id":"learn-20260205-0001","timestamp":"2026-02-05T12:00:00Z","summary":"NEVER hand-roll parsers (markdown, HTML escape, etc.) inside JavaScript template literals—use established CDN libraries instead","trigger":"friction","impact":"Hand-rolled regex inside TypeScript template literals caused TWO fatal bugs: (1) \\n in code block regex became a literal newline causing JS SyntaxError, (2) \\* in bold/italic regex was silently eaten producing /* which JS parsed as block comment start. Both killed the entire chat UI script—no event listeners attached, submit button completely inert. Switching to marked@15 + DOMPurify@3 from CDN eliminated all regex-in-template-literal issues permanently.","confidence":1.0,"sources":["workers/src/chat-ui.ts","session-conversation-2026-02-05"],"evidence":[{"type":"commit","ref":"40e06a9 - Fix fatal JS parse error in chat UI code block regex"},{"type":"commit","ref":"f49f757 - Fix bold/italic regex escaping in template literal"},{"type":"commit","ref":"e52ba0f - Replace hand-rolled markdown with marked + DOMPurify from CDN"}],"candidate_targets":["klappy://canon/constraints"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260205-0002","timestamp":"2026-02-05T12:01:00Z","summary":"Cloudflare Worker secrets set via wrangler secret put can be wiped by deploys—use Cloudflare dashboard (Workers > Settings > Variables and Secrets > Encrypt) for persistent secrets in cloud/CI environments","trigger":"friction","impact":"OPENAI_API_KEY set via wrangler secret put was wiped on every deploy, causing repeated 'API key not configured' errors. The --var flag in deploy scripts requires local env vars which are unavailable in cloud environments (Claude Code on web). The only reliable path for secrets in cloud CI is the Cloudflare dashboard encrypt option.","confidence":0.95,"sources":["workers/wrangler.toml","session-conversation-2026-02-05"],"evidence":[{"type":"commit","ref":"64c8e26 - Revert --var OPENAI_API_KEY from deploy, use dashboard secret"},{"type":"artifact","ref":"workers/wrangler.toml - comment documenting dashboard secret path"}],"candidate_targets":["oddkit://docs/deployment"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260205-0003","timestamp":"2026-02-05T12:02:00Z","summary":"NEVER assume the user's deployment environment is local—they may be in a cloud environment where they cannot set env vars, access local files, or commit secrets to a public repo","trigger":"friction","impact":"Attempted to solve OPENAI_API_KEY persistence by injecting via --var from shell environment. User was in Claude Code on the web—a cloud environment with no persistent local env vars. This caused significant frustration. Always ask about the deployment environment before proposing env-var-based solutions.","confidence":1.0,"sources":["session-conversation-2026-02-05"],"evidence":[{"type":"artifact","ref":"workers/package.json - deploy script reverted to original"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260205-0004","timestamp":"2026-02-05T12:03:00Z","summary":"Use the EXACT model name the user specifies—do not substitute or 'correct' it based on assumptions about what models exist","trigger":"friction","impact":"User requested gpt-5.2-mini but implementation used o4-mini based on assumption about available OpenAI models. User caught this in GitHub review. The user knows what model they want; the agent's job is to implement what was asked, not second-guess model availability.","confidence":1.0,"sources":["workers/src/chat-api.ts","session-conversation-2026-02-05"],"evidence":[{"type":"artifact","ref":"workers/src/chat-api.ts - MODEL = 'gpt-5.2-mini'"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260205-0005","timestamp":"2026-02-05T12:04:00Z","summary":"Client-side error display must show actual server error messages, not guess based on HTTP status codes","trigger":"friction","impact":"Chat UI hardcoded error messages like 'API key not configured' for 500 status, instead of reading the actual error body from the server. This made debugging impossible—the real error was hidden. Always parse and display res.json().error or res.json().detail from the response body.","confidence":0.95,"sources":["workers/src/chat-ui.ts","session-conversation-2026-02-05"],"evidence":[{"type":"commit","ref":"3041529 - Show actual API error instead of guessing from status code"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260205-0006","timestamp":"2026-02-05T12:05:00Z","summary":"Security-first for chat APIs: filter client messages to user/assistant roles only, validate body schema, restrict markdown link URLs to http/https, sanitize HTML output with DOMPurify","trigger":"policy","impact":"Multiple XSS and injection vectors were found in GitHub review: (1) client could inject system messages, (2) markdown javascript: URLs enabled XSS, (3) unescaped quotes in HTML attributes. All fixed by: role filtering, custom marked renderer with URL scheme whitelist, DOMPurify sanitization with ADD_ATTR for target.","confidence":0.95,"sources":["workers/src/chat-api.ts","workers/src/chat-ui.ts","session-conversation-2026-02-05"],"evidence":[{"type":"commit","ref":"8907d9a - Fix newline loss, javascript: URL XSS, and system message injection"},{"type":"commit","ref":"738dd7f - Fix review bugs: body validation, dead variable, XSS in markdown"}],"candidate_targets":["klappy://canon/constraints"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260206-0001","timestamp":"2026-02-06T00:00:00Z","summary":"OpenAI mini model is gpt-5-mini NOT gpt-5.2-mini — always verify model IDs against live OpenAI docs, not training data","trigger":"friction","impact":"gpt-5.2-mini does not exist in the OpenAI API — returns model_not_found error. The mini variant is from the GPT-5 family (gpt-5-mini). GPT-5.2 exists as a flagship but has no mini variant. Previous learning learn-20260205-0004 said use exact user names, but the user's requested name was also wrong. Must verify against live docs.","confidence":1.0,"sources":["https://platform.openai.com/docs/models","session-conversation-2026-02-06"],"evidence":[{"type":"artifact","ref":"OpenAI API error: The model gpt-5.2-mini does not exist"},{"type":"commit","ref":"5edf5e7 - Fix model name: gpt-5.2-mini does not exist, use gpt-5-mini"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260206-0002","timestamp":"2026-02-06T01:00:00Z","summary":"NEVER use markdown bold/italic (** or *) when giving the user links or text to paste — it breaks URLs and copy-paste","trigger":"friction","impact":"User asked for a PR link multiple times. Each time the response included ** markdown formatting which corrupts URLs and text when pasted into GitHub or browsers. The user had to ask again WITHOUT formatting. Plain text only for anything the user needs to copy.","confidence":1.0,"sources":["session-conversation-2026-02-06"],"evidence":[{"type":"artifact","ref":"User feedback: WITHOUT ** it breaks the links!!!"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260206-0003","timestamp":"2026-02-06T02:00:00Z","summary":"wrangler deploy with [vars] section nukes ALL dashboard-set env vars not listed in [vars] — must add keep_vars = true to wrangler.toml","trigger":"friction","impact":"Every single deploy wiped the OPENAI_API_KEY because wrangler treats [vars] as the complete set of variables. Dashboard-set secrets not in [vars] get deleted. User had to re-add the key a dozen times. The fix is keep_vars = true in wrangler.toml which tells wrangler to preserve dashboard variables not in the toml. Previous learning learn-20260205-0002 was WRONG — it blamed wrangler secret put but the real cause was the [vars] section overriding everything on deploy.","confidence":1.0,"sources":["https://developers.cloudflare.com/workers/wrangler/configuration/","https://community.cloudflare.com/t/keep-vars-not-working-as-expected/831643","session-conversation-2026-02-06"],"evidence":[{"type":"commit","ref":"f35b405 - Add keep_vars = true to stop deploys from nuking dashboard secrets"},{"type":"artifact","ref":"workers/wrangler.toml - keep_vars = true on line 8"}],"candidate_targets":[],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260206-0004","timestamp":"2026-02-06T03:00:00Z","summary":"GPT-5 family models require max_completion_tokens, not max_tokens — always RTFM on API parameter names","trigger":"friction","impact":"GPT-5 models reject the deprecated max_tokens parameter with a 400 error. The correct parameter is max_completion_tokens which accounts for both reasoning tokens and visible output tokens. This applies to all GPT-5.x, o-series, and GPT-4o models. Only legacy models (GPT-3.5, older GPT-4) accept max_tokens. Should have checked the live API docs before using the parameter.","confidence":1.0,"sources":["https://platform.openai.com/docs/api-reference/chat","https://community.openai.com/t/why-was-max-tokens-changed-to-max-completion-tokens/938077","session-conversation-2026-02-06"],"evidence":[{"type":"artifact","ref":"OpenAI API error: Unsupported parameter max_tokens, use max_completion_tokens"},{"type":"commit","ref":"e1b11d6 - Fix OpenAI API param: GPT-5 requires max_completion_tokens not max_tokens"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260206-0005","timestamp":"2026-02-06T04:00:00Z","summary":"When fixing one API param, check ALL params against live docs — gpt-5-mini also rejects temperature!=1","trigger":"friction","impact":"Fixed max_tokens to max_completion_tokens but left temperature:0.7 which gpt-5-mini also rejects. When you find one unsupported param, check every param in the request against live docs. Don't fix one and leave the rest unchecked. GPT-5 mini only supports temperature=1 (default).","confidence":1.0,"sources":["https://platform.openai.com/docs/api-reference/chat","session-conversation-2026-02-06"],"evidence":[{"type":"artifact","ref":"OpenAI API error: temperature does not support 0.7 with this model"},{"type":"commit","ref":"d7a2b35 - Remove temperature param — gpt-5-mini only supports default (1)"}],"candidate_targets":[],"proposed_escalation":"none"}
{"id":"learn-20260206-0006","timestamp":"2026-02-06T05:00:00Z","summary":"Never blindly inject RAG context on every chat turn — use LLM-driven function calling so the model decides when and what to retrieve","trigger":"friction","impact":"Blind context injection (oddkit enrichment on every message) caused cascading confusion: each turn accumulated verbose documentation in conversation history, model re-answered prior questions, lost track of what it already covered, and collapsed by turn 3. Replacing with OpenAI function calling lets the LLM decide IF it needs docs (skip for follow-ups) and WHAT to query (focused query vs raw user text). Also requires: (1) trimming long assistant messages in history to prevent context bloat, (2) system prompt instruction to focus on latest message only.","confidence":1.0,"sources":["workers/src/chat-api.ts","session-conversation-2026-02-06"],"evidence":[{"type":"artifact","ref":"workers/src/chat-api.ts - replaced getOddkitContext with ODDKIT_TOOL function calling"}],"candidate_targets":["klappy://canon/constraints"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260207-0001","timestamp":"2026-02-07T00:00:00Z","summary":"MCP tool discoverability requires strict spec compliance: 202 (not 204) for notifications, 405 (not JSON) for GET without SSE, serverInfo must not contain protocolVersion, CORS must include Authorization","trigger":"friction","impact":"ChatGPT agent builder and other MCP clients failed to discover tools because: (1) notifications/initialized got 204 instead of spec-mandated 202, causing clients to bail before tools/list; (2) GET /mcp returned JSON info page instead of 405, confusing clients into thinking it was legacy HTTP+SSE transport; (3) serverInfo contained extra protocolVersion field violating Implementation schema, causing strict validation failures; (4) CORS missing Authorization header blocked ChatGPT from sending auth tokens. Fix: 204→202, GET→405, remove protocolVersion from serverInfo, add Authorization to CORS, add instructions field to InitializeResult.","confidence":0.95,"sources":["workers/src/index.ts","spec.modelcontextprotocol.io/specification/2025-03-26/basic/transports/","platform.openai.com/docs/mcp"],"evidence":[{"type":"artifact","ref":"workers/src/index.ts - notification handler, GET handler, serverInfo, corsHeaders"}],"candidate_targets":["klappy://canon/constraints"],"proposed_escalation":"candidate-constraint"}
{"id":"learn-20260207-0002","timestamp":"2026-02-07T06:00:00Z","summary":"OpenAI Agent Builder 424 has THREE root causes: (1) missing serverInfo.version when env var undefined, (2) POST must return SSE not JSON when client sends Accept: text/event-stream, (3) batch JSON-RPC arrays must be handled","trigger":"friction","impact":"After 3 PRs fixing MCP spec compliance, ChatGPT agent builder still returned 424 Failed Dependency. Root causes: (1) ODDKIT_VERSION env var was undefined in production — JSON.stringify silently drops undefined fields, so serverInfo had no version field, violating the MCP Implementation schema (name+version both required). Strict validation rejects this. (2) OpenAI sends Accept: application/json, text/event-stream and expects SSE responses for POST (text/event-stream Content-Type with event: message + data: lines). Server was only returning application/json. Multiple community reports confirm SSE is required. (3) MCP spec allows batch requests (JSON arrays of messages) but server only handled single messages — Array.isArray check was missing, causing method:undefined errors. Fix: default version fallback, SSE responses when Accept includes text/event-stream, batch request loop with per-message response collection.","confidence":0.9,"sources":["workers/src/index.ts","community.openai.com/t/mcp-server-passes-all-json-rpc-tests-but-agent-builder-fails-with-424","github.com/jlowin/fastmcp/issues/855","modelcontextprotocol.io/specification/2025-03-26/basic/transports"],"evidence":[{"type":"artifact","ref":"curl -v oddkit.klappy.dev/mcp — serverInfo:{name:oddkit} with NO version field"},{"type":"artifact","ref":"workers/src/index.ts — POST handler returned application/json, never text/event-stream"}],"candidate_targets":["klappy://canon/constraints"],"proposed_escalation":"candidate-constraint"}
